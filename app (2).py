# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PtTA2_GPwYmaT-cbNeoO51_GgwstSxcb
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from scipy.stats import uniform

# Page title
st.title("ðŸ”§ ML Model Hyperparameter Tuning App")
st.write("Train multiple machine learning models, tune hyperparameters, and compare their performance.")

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Split and scale data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Parameter grids
param_grids = {
    "Logistic Regression": {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear']
    },
    "Random Forest": {
        'n_estimators': [50, 100, 150],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 5]
    },
    "SVM": {
        'C': uniform(0.1, 10),
        'kernel': ['linear', 'rbf', 'poly'],
        'gamma': ['scale', 'auto']
    },
    "KNN": {
        'n_neighbors': [3, 5, 7, 9],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    }
}

# Tune models
@st.cache_data(show_spinner=True)
def tune_model(name):
    if name == "Logistic Regression":
        model = LogisticRegression()
        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1')
    elif name == "Random Forest":
        model = RandomForestClassifier()
        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1')
    elif name == "SVM":
        model = SVC()
        grid = RandomizedSearchCV(model, param_distributions=param_grids[name], n_iter=20, cv=5, scoring='f1', random_state=42)
    elif name == "KNN":
        model = KNeighborsClassifier()
        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1')
    else:
        return None

    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)

    return {
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1 Score': f1_score(y_test, y_pred),
        'Best Params': grid.best_params_,
        'Predictions': y_pred,
        'Estimator': best_model
    }

# Run tuning
model_names = ["Logistic Regression", "Random Forest", "SVM", "KNN"]
results = []
with st.spinner("Training and tuning models..."):
    for name in model_names:
        result = tune_model(name)
        results.append(result)

# Display metrics
st.subheader("ðŸ“Š Model Performance")
df_results = pd.DataFrame(results)[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']]
st.dataframe(df_results)

# Bar Chart
st.subheader("ðŸ“ˆ Model Comparison Chart")
df_plot = df_results.set_index('Model')
st.bar_chart(df_plot)

# Best model
best_model_result = max(results, key=lambda x: x['F1 Score'])
best_model_name = best_model_result['Model']
st.success(f"âœ… Best model based on F1 Score: {best_model_name}")

# Confusion Matrix
st.subheader("ðŸ”² Confusion Matrix")
cm = confusion_matrix(y_test, best_model_result['Predictions'])
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names, ax=ax)
ax.set_xlabel("Predicted")
ax.set_ylabel("Actual")
st.pyplot(fig)

# Best hyperparameters
st.subheader("ðŸ§ª Best Hyperparameters")
for r in results:
    st.markdown(f"**{r['Model']}**")
    st.json(r['Best Params'])

